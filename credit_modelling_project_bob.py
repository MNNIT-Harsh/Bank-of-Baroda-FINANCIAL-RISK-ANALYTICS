# -*- coding: utf-8 -*-
"""Credit_Modelling_Project_BOB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u1V9AGhs2oMSz49zJD4_UkfED0mB-Qav
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score
from scipy.stats import chi2_contingency
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support
import warnings
import os

"""UPLOADING THE FILES

CASE STUDY 1 ---- INTERNAL PRODUCT FILE [BANK OF BARODA]
CASE STUDY 2 ---- CIBIL REPORT FOR SAME DATASET [BANK OF BARODA]


"""

a1 = pd.read_excel('/content/case_study1.xlsx')
a2 = pd.read_excel('/content/case_study2.xlsx')

"""Creating copies of both the dataframes"""

df1 = a1.copy()
df2 = a2.copy()

"""Analysis of dataframe"""

df1.describe()

df2.describe()

df1.info()

df2.info()

df1.head()

df2.head()

"""â–¶ DATA CLEANING STARTS:

   ðŸŽ¯TARGET?
    
*   TO ENSURE THAT THERE ARE NO NULL VALUES.
*   TO JOIN THE TWO DATAFRAMES BASED ON COMMON PARAMETER.


"""

df1.shape

"""writing the code snippet to remove the null values, here the null values are represented by -99999.
so, we will be dropping the rows which cintains the null values as they are only 40 in number for the case study 1 dataset
"""

df1 = df1.loc[df1['Age_Oldest_TL']!= -99999]

df1.shape

"""Now, we will be handling the case study 2 datafarme , note that it is the CIBIL report and it contains many null values which exceeds almost 70% percent of the total value of a particular coloumn so we will iterate through the rows and prefer dropping those which have null value count more than 10000 and we will drop those rows where where null value count is less than 10000.

Also, imputation wont be a good approach as we are dealing with sensitive bank information.
"""

coloumns_to_drop = []
for i in df2.columns:
  if df2.loc[df2[i] == -99999].shape[0] > 10000:  # Access the first element of the shape tuple
    coloumns_to_drop.append(i)

coloumns_to_drop

df2 = df2.drop(coloumns_to_drop, axis = 1)

df2.info()

df2.shape

for i in df2.columns:
  df2 = df2.loc[df2[i] != -99999]

df2.shape

df1.isnull().sum()

df2.isnull().sum()

"""looking for a common parameter to merge"""

for i in list (df1.columns):
  if i in list (df2.columns):
    print(i)

"""Now, Merging the two dataframes based on PROSPECT_ID"""

df = pd.merge(df1, df2, how ='inner' , on = 'PROSPECTID')

df.describe()

df.info()

df.head()

"""Checking for Null Values"""

df.isnull().sum()

"""**DATA CLEANING COMPLETED AND BOTH TASKS HAVE BEEN ACHIEVED SUCCESSFULLY!**

NOW,

**FEATURE ENGINEERING STARTS ----STEP 2**
 **ðŸŽ¯TARGET?**


*   WE WILL CATEGORISE THE DATA MAINLY INTO TWO CATEGORIES

1.   NUMERICAL
2.   OBJECT TYPE


*   DEAL WITH BOTH TYPES SEPERATELY
"""

#CHECKING HOW MANY COLOUMNS ARE CATEGORICAL

for i in df.columns:
  if df[i].dtype == 'object':
    print(i)

df['MARITALSTATUS'].value_counts()

df['EDUCATION'].value_counts()

df['GENDER'].value_counts()

df['last_prod_enq2'].value_counts()

df['first_prod_enq2'].value_counts()

df['Approved_Flag'].value_counts()



"""Okay!
Now understand that since we are doing feature engineering so typically we **wont consider the approved flag part** because it actually holds up the target variable for this **multi class classification**.
Hence, we will just analyse the remaining four categories for our **FEATURE ENGINEERING PROSPECT**!

IN THE PART OF FEATURE ENGINEERING WE WILL DEAL WITH A BIT OF INFERENTIAL STATISTICS:

1. CHI SQUARE TEST-----CAT VS CAT
2. T- TEST ---------CAT VS NUM-----[2 CATEGORIES]
3. ANOVA TEST ----- CAT VS NUM-----[>=3 CATEGORIES]
"""

for i in ['MARITALSTATUS','EDUCATION','GENDER','last_prod_enq2','first_prod_enq2']:
    chi2, pval, _, _ = chi2_contingency(pd.crosstab(df['Approved_Flag'], df[i]))
    print(i, '----', pval)

"""NOW, SINCE ALL THE **VALUES MENTIONED ARE LESS THAN THE P-VALUE** THAT WAS ASSUMED AS **0.05**.
SO WE **CAN NOT REJECT THE HYPOTHESIS!**
"""



"""NOW LET US ANALYZE THE **NUMERICAL** **FEATURE**"""

numeric_columns =[]
for i in df.columns:
  if df[i].dtype != 'object' and i not in ['PROSPECTID','Approved_Flag']:
    numeric_columns.append(i)

print(numeric_columns)

"""VARIATION INFLATION FACTOR CHECK"""

vif_data = df[numeric_columns]
total_columns = vif_data.shape[1]
columns_to_be_kept = []
column_index = 0



for i in range (0,total_columns):

    vif_value = variance_inflation_factor(vif_data, column_index)
    print (column_index,'---',vif_value)


    if vif_value <= 6:
        columns_to_be_kept.append( numeric_columns[i] )
        column_index = column_index+1

    else:
        vif_data = vif_data.drop([ numeric_columns[i] ] , axis=1)

"""OKAY!

NOW NOTICE THAT AFTER THIS STEP WE HAD **72 COLOUMNS** AND EVENTUALLY AFTER VIF TEST WE ARE LEFT WITH **39 EFFECTIVE COLUMNNS**

NOW WE ARE GOOD TO GO FOR **ANOVA TEST**
"""

from scipy.stats import f_oneway

columns_to_be_kept_numerical = []

for i in columns_to_be_kept:
    a = list(df[i])
    b = list(df['Approved_Flag'])

    group_P1 = [value for value, group in zip(a, b) if group == 'P1']
    group_P2 = [value for value, group in zip(a, b) if group == 'P2']
    group_P3 = [value for value, group in zip(a, b) if group == 'P3']
    group_P4 = [value for value, group in zip(a, b) if group == 'P4']


    f_statistic, p_value = f_oneway(group_P1, group_P2, group_P3, group_P4)

    if p_value <= 0.05:
        columns_to_be_kept_numerical.append(i)

"""NOW WE WILL DO FEATURE SELECTION WILL BE DONE

LET US DESIGN THE ORDINAL FEATURES NOW

1. SSC---1
2. UNDER GRAD-----3
3. GRADUATE-----3
4. POST GRAD-----4
5. OTHERS-----1
6. PROFESSIONAL---4
7. 12TH ------2
"""

#OTHERS SECTION NEEDS TO VERIFIED


df.loc[df['EDUCATION'] == 'SSC',['EDUCATION']]              = 1
df.loc[df['EDUCATION'] == '12TH',['EDUCATION']]             = 2
df.loc[df['EDUCATION'] == 'GRADUATE',['EDUCATION']]         = 3
df.loc[df['EDUCATION'] == 'UNDER GRADUATE',['EDUCATION']]   = 3
df.loc[df['EDUCATION'] == 'POST-GRADUATE',['EDUCATION']]    = 4
df.loc[df['EDUCATION'] == 'OTHERS',['EDUCATION']]           = 1
df.loc[df['EDUCATION'] == 'PROFESSIONAL',['EDUCATION']]     = 3

df['EDUCATION'].value_counts()

df['EDUCATION'] = df['EDUCATION'].astype(int)

df.info()

df_encoded = pd.get_dummies(df, columns=['MARITALSTATUS','GENDER', 'last_prod_enq2' ,'first_prod_enq2'])

df_encoded.info()

k = df_encoded.describe()

k

df_encoded.head()

df_encoded.info()



"""**OKAY THIS MARKS THE COMPLETION OF FEATURE ENGINEERING NOW WE WILL START WITH THE MODEL SELECTION PART**

**MACHINE LEARNING MODEL SELECTION**

# Data processing

# 1. Random Forest
"""

y = df_encoded['Approved_Flag']
x = df_encoded. drop ( ['Approved_Flag'], axis = 1 )




x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)





rf_classifier = RandomForestClassifier(n_estimators = 200, random_state=42)





rf_classifier.fit(x_train, y_train)



y_pred = rf_classifier.predict(x_test)





accuracy = accuracy_score(y_test, y_pred)
print ()
print(f'Accuracy: {accuracy}')
print ()
precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred)


for i, v in enumerate(['p1', 'p2', 'p3', 'p4']):
    print(f"Class {v}:")
    print(f"Precision: {precision[i]}")
    print(f"Recall: {recall[i]}")
    print(f"F1 Score: {f1_score[i]}")
    print()

"""#XG BOOST"""

import xgboost as xgb
from sklearn.preprocessing import LabelEncoder

xgb_classifier = xgb.XGBClassifier(objective='multi:softmax',  num_class=4)



y = df_encoded['Approved_Flag']
x = df_encoded. drop ( ['Approved_Flag'], axis = 1 )


label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)


x_train, x_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=42)




xgb_classifier.fit(x_train, y_train)
y_pred = xgb_classifier.predict(x_test)

accuracy = accuracy_score(y_test, y_pred)
print ()
print(f'Accuracy: {accuracy:.2f}')
print ()

precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred)

for i, v in enumerate(['p1', 'p2', 'p3', 'p4']):
    print(f"Class {v}:")
    print(f"Precision: {precision[i]}")
    print(f"Recall: {recall[i]}")
    print(f"F1 Score: {f1_score[i]}")
    print()

"""#DECISION TREE

"""

from sklearn.tree import DecisionTreeClassifier


y = df_encoded['Approved_Flag']
x = df_encoded. drop ( ['Approved_Flag'], axis = 1 )

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)


dt_model = DecisionTreeClassifier(max_depth=20, min_samples_split=10)
dt_model.fit(x_train, y_train)
y_pred = dt_model.predict(x_test)

accuracy = accuracy_score(y_test, y_pred)
print ()
print(f"Accuracy: {accuracy:.2f}")
print ()

precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred)

for i, v in enumerate(['p1', 'p2', 'p3', 'p4']):
    print(f"Class {v}:")
    print(f"Precision: {precision[i]}")
    print(f"Recall: {recall[i]}")
    print(f"F1 Score: {f1_score[i]}")
    print()